{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00725cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import playsound\n",
    "import speech_recognition as sr\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db41884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_speech_from_mic():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        audio = recognizer.listen(source, timeout=5)\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio)\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            return \"Google Speech Recognition could not understand audio\"\n",
    "        except sr.RequestError:\n",
    "            return \"Could not request results from Google Speech Recognition service\"\n",
    "\n",
    "def get_response_from_openai(text, client):\n",
    "    response = client.chat.completions.create(\n",
    "        model= \"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": text}]\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "def text_to_speech(text, client):\n",
    "    speech_file_path = Path(__file__).parent / \"speech.mp3\"\n",
    "    print(\"speech_file_path:\", speech_file_path)\n",
    "    try:\n",
    "        speech_response = client.audio.speech.create(\n",
    "            model=\"tts-1\",\n",
    "            voice=\"alloy\",\n",
    "            input=text\n",
    "        )\n",
    "        speech_response.stream_to_file(str(speech_file_path))\n",
    "        playsound.playsound(str(speech_file_path))\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred in text-to-speech:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a75a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(api_key='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')\n",
    "# Step 1: Create an Assistant\n",
    "# assistant = client.beta.assistants.create(...)  # Fill with your parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19999b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread: Thread(id='thread_nO2WsFgkUNVIhVd8YWgrVeha', created_at=1700534449, metadata={}, object='thread') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Start a conversation thread\n",
    "\n",
    "thread = client.beta.threads.create()\n",
    "print(\"thread:\", thread, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98a1f205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Recognized Speech: hello\n"
     ]
    }
   ],
   "source": [
    "# Speech recognition\n",
    "speech_text = recognize_speech_from_mic()\n",
    "print(\"Recognized Speech:\", speech_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "260f9167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message: ThreadMessage(id='msg_Pn28XkIGHxuA2jOXVk41qUYv', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='hello'), type='text')], created_at=1700534500, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_nO2WsFgkUNVIhVd8YWgrVeha') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Add user's message to the thread\n",
    "message = client.beta.threads.messages.create(thread_id=thread.id, role=\"user\", content=speech_text)\n",
    "print(\"message:\", message, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17438bf9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'assistant' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 4: Run the Assistant\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m run \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mthreads\u001b[38;5;241m.\u001b[39mruns\u001b[38;5;241m.\u001b[39mcreate(thread_id\u001b[38;5;241m=\u001b[39mthread\u001b[38;5;241m.\u001b[39mid, assistant_id\u001b[38;5;241m=\u001b[39m\u001b[43massistant\u001b[49m\u001b[38;5;241m.\u001b[39mid)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'assistant' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 4: Run the Assistant\n",
    "run = client.beta.threads.runs.create(thread_id=thread.id, assistant_id=\"asst_dQRSgumYCUkH0MEf9vIFMl6V\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90471f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d2e5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab220796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ec_venv",
   "language": "python",
   "name": "ec_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
